{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Input,Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Concatenate,Conv1D,MaxPool1D,Dropout\n",
    "\n",
    "from src.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_movie_conv = \"cornell movie-dialogs corpus/movie_conversations.txt\"\n",
    "corpus_movie_lines = \"cornell movie-dialogs corpus/movie_lines.txt\"\n",
    "max_len = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2Model.\n",
      "\n",
      "All the layers of TFGPT2Model were initialized from the model checkpoint at microsoft/DialoGPT-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFBertModel.\n",
      "\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at cambridgeltl/BioRedditBERT-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "dialogpt_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "dialogpt_model = TFAutoModel.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "\n",
    "biobert_tokenizer = AutoTokenizer.from_pretrained(\"cambridgeltl/BioRedditBERT-uncased\")\n",
    "biobert_model = TFAutoModel.from_pretrained(\"cambridgeltl/BioRedditBERT-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_chat_data():\n",
    "    file = os.path.join(Config.FILES[\"RAW_DATA_DIR\"], \"chat.json\")\n",
    "    with open(file) as f:\n",
    "        data = json.loads(f.read())\n",
    "        \n",
    "    parsed_dials = []\n",
    "    for line in data:\n",
    "        tags = line[\"topic\"]\n",
    "        dialogue = line[\"dialogue\"]\n",
    "        \n",
    "        all_tags = []\n",
    "        all_emotion = []\n",
    "        all_text = []\n",
    "        for content in dialogue:\n",
    "            text = content[\"text\"]\n",
    "            all_text.append(text)\n",
    "            emotion = content[\"emotion\"]\n",
    "            all_emotion.append(emotion)\n",
    "            all_tags.append(tags)\n",
    "\n",
    "        parsed_dials.append({\"persona1\": [], \"persona2\": [], \"turns\": all_text, \"emotions\": all_emotion, \"tags\": all_tags})\n",
    "        dial_sent_lens = [len(s) for turn in parsed_dials for s in turn[\"turns\"]]\n",
    "        \n",
    "    dialogues_df = pd.DataFrame()\n",
    "    for turns in parsed_dials:\n",
    "        temp_df = pd.DataFrame()\n",
    "        temp_turns = turns[\"turns\"]\n",
    "        tuples_turns_list = [(temp_turns[i], temp_turns[i+1]) for i in range(0, len(temp_turns)-1, 2)]\n",
    "        temp_turns_df = pd.DataFrame(tuples_turns_list, columns=[\"questions\", \"answers\"])\n",
    "        \n",
    "        temp_emotions = turns[\"emotions\"]\n",
    "        tuples_emotions_list = [(temp_emotions[i], temp_emotions[i+1]) for i in range(0, len(temp_emotions)-1, 2)]\n",
    "        temp_emotions_df = pd.DataFrame(tuples_emotions_list, columns=[\"questions_emotions\", \"answers_emotions\"])\n",
    "        \n",
    "        temp_tags = turns[\"tags\"]\n",
    "        tuples_tags_list = [(temp_tags[i], temp_tags[i+1]) for i in range(0, len(temp_tags)-1, 2)]\n",
    "        temp_tags_df = pd.DataFrame(tuples_tags_list, columns=[\"questions_tags\", \"answers_tags\"])\n",
    "        \n",
    "        temp_df = pd.concat([temp_turns_df, temp_emotions_df, temp_tags_df], axis=1)\n",
    "        dialogues_df = pd.concat([dialogues_df, temp_df])\n",
    "        \n",
    "    dialogues_df = dialogues_df.reset_index(drop=True)\n",
    "    dialogues_df[\"emotions\"] = dialogues_df.apply(lambda row: [row[\"questions_emotions\"], row[\"answers_emotions\"]], axis=1)\n",
    "    dialogues_df[\"tags\"] = dialogues_df.apply(lambda row: [row[\"questions_tags\"], row[\"answers_tags\"]], axis=1)\n",
    "    \n",
    "    dialogues_df[\"tags_encode\"] = dialogues_df[\"tags\"].apply(lambda x: _extract_tags(x))\n",
    "    dialogues_df[\"label\"] = dialogues_df[\"tags_encode\"].apply(lambda x: _tags_encoder(x))\n",
    "        \n",
    "    return data, parsed_dials, dialogues_df\n",
    "\n",
    "\n",
    "def decontractions(phrase):\n",
    "    \"\"\"decontracted takes text and convert contractions into natural form.\n",
    "     ref: https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python/47091490#47091490\"\"\"\n",
    "    phrase = re.sub(r\"won\\\"t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"won\\’t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\’t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"n\\\"t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\\"re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\\"s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\\"d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\\"ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\\"t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\\"ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\\"m\", \" am\", phrase)\n",
    "\n",
    "    phrase = re.sub(r\"n\\’t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\’re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\’s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\’d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\’ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\’t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\’ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\’m\", \" am\", phrase)\n",
    "\n",
    "    return phrase\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    # convert all the text into lower letters\n",
    "    # remove the words betweent brakets ()\n",
    "    # remove these characters: {\"$\", ')', '?', '\"', '’', '.',  '°', '!', ';', '/', \"'\", '€', '%', ':', ',', '('}\n",
    "    # replace these spl characters with space: '\\u200b', '\\xa0', '-', '/'\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = decontractions(text)\n",
    "    text = re.sub('[$)\\?\"’.°!;\\'€%:,(/]', \"\", text)\n",
    "    text = re.sub(\"\\u200b\", \" \", text)\n",
    "    text = re.sub(\"\\xa0\", \" \", text)\n",
    "    text = re.sub(\"-\", \" \", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_chat_data():\n",
    "    chat_data, parsed_dials, dialogues_df = parse_chat_data()\n",
    "    \n",
    "    dialogues_df[\"preprocessed_question\"] = dialogues_df[\"questions\"].apply(preprocess)\n",
    "    dialogues_df[\"preprocessed_answer\"] = dialogues_df[\"answers\"].apply(preprocess)\n",
    "    \n",
    "    dialogues_df[\"question_len\"] = dialogues_df[\"preprocessed_question\"].apply(lambda x: len(x.split(\" \")))\n",
    "    dialogues_df[\"answer_len\"] = dialogues_df[\"preprocessed_answer\"].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "    dialogues_df[\"short_question\"] = dialogues_df.apply(lambda x: \" \".join(x.preprocessed_question.split(\" \")[:500]) if x.question_len>500 else x.preprocessed_question ,axis=1)\n",
    "    dialogues_df[\"short_answer\"] = dialogues_df.apply(lambda x: \" \".join(x.preprocessed_answer.split(\" \")[:500]) if x.answer_len>500 else x.preprocessed_answer ,axis=1)\n",
    "        \n",
    "    return chat_data, parsed_dials, dialogues_df\n",
    "\n",
    "\n",
    "def extract_negative_samples(question, tags):\n",
    "  stop=False\n",
    "  while (not stop):\n",
    "    sample_row = dialogues_df.sample()\n",
    "    sample_tags = dialogues_df[\"tags\"].values[0]\n",
    "    inter_tags = set(tags[0]).intersection(set(sample_tags))\n",
    "    \n",
    "    if len(inter_tags)==0:\n",
    "      stop=True\n",
    "  \n",
    "  return sample_row\n",
    "\n",
    "\n",
    "def _extract_tags(tags_list):\n",
    "    if all(x == tags_list[0] for x in tags_list):\n",
    "        return tags_list[0]\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "    \n",
    "def _tags_encoder(tags):\n",
    "    if tags == \"greeting\":\n",
    "        return 1\n",
    "    elif tags == \"delivery\":\n",
    "        return 2\n",
    "    elif tags == \"complaint\":\n",
    "        return 3\n",
    "    elif tags == \"credit_card\":\n",
    "        return 4\n",
    "    elif tags == \"loan\":\n",
    "        return 5\n",
    "    elif tags == \"insurance\":\n",
    "        return 6\n",
    "    elif tags == \"remittance\":\n",
    "        return 7\n",
    "    elif tags == \"attitude_and_emotion\":\n",
    "        return 8\n",
    "    elif tags == \"relationship\":\n",
    "        return 9\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_data, parsed_dials, dialogues_df = get_chat_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questions</th>\n",
       "      <th>answers</th>\n",
       "      <th>questions_emotions</th>\n",
       "      <th>answers_emotions</th>\n",
       "      <th>questions_tags</th>\n",
       "      <th>answers_tags</th>\n",
       "      <th>emotions</th>\n",
       "      <th>tags</th>\n",
       "      <th>tags_encode</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed_question</th>\n",
       "      <th>preprocessed_answer</th>\n",
       "      <th>question_len</th>\n",
       "      <th>answer_len</th>\n",
       "      <th>short_question</th>\n",
       "      <th>short_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi</td>\n",
       "      <td>Hi there, I am RAK-Voice, what can I do for you?</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>greeting</td>\n",
       "      <td>greeting</td>\n",
       "      <td>[neutral, neutral]</td>\n",
       "      <td>[greeting, greeting]</td>\n",
       "      <td>greeting</td>\n",
       "      <td>1</td>\n",
       "      <td>hi</td>\n",
       "      <td>hi there i am rak voice what can i do for you</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>hi</td>\n",
       "      <td>hi there i am rak voice what can i do for you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RAK-Voice, this is the first time I knowing yo...</td>\n",
       "      <td>Glad that you ask this questions, I am an inte...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>greeting</td>\n",
       "      <td>greeting</td>\n",
       "      <td>[neutral, neutral]</td>\n",
       "      <td>[greeting, greeting]</td>\n",
       "      <td>greeting</td>\n",
       "      <td>1</td>\n",
       "      <td>rak voice this is the first time i knowing you...</td>\n",
       "      <td>glad that you ask this questions i am an intel...</td>\n",
       "      <td>13</td>\n",
       "      <td>33</td>\n",
       "      <td>rak voice this is the first time i knowing you...</td>\n",
       "      <td>glad that you ask this questions i am an intel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           questions  \\\n",
       "0                                                 Hi   \n",
       "1  RAK-Voice, this is the first time I knowing yo...   \n",
       "\n",
       "                                             answers questions_emotions  \\\n",
       "0   Hi there, I am RAK-Voice, what can I do for you?            neutral   \n",
       "1  Glad that you ask this questions, I am an inte...            neutral   \n",
       "\n",
       "  answers_emotions questions_tags answers_tags            emotions  \\\n",
       "0          neutral       greeting     greeting  [neutral, neutral]   \n",
       "1          neutral       greeting     greeting  [neutral, neutral]   \n",
       "\n",
       "                   tags tags_encode  label  \\\n",
       "0  [greeting, greeting]    greeting      1   \n",
       "1  [greeting, greeting]    greeting      1   \n",
       "\n",
       "                               preprocessed_question  \\\n",
       "0                                                 hi   \n",
       "1  rak voice this is the first time i knowing you...   \n",
       "\n",
       "                                 preprocessed_answer  question_len  \\\n",
       "0      hi there i am rak voice what can i do for you             1   \n",
       "1  glad that you ask this questions i am an intel...            13   \n",
       "\n",
       "   answer_len                                     short_question  \\\n",
       "0          12                                                 hi   \n",
       "1          33  rak voice this is the first time i knowing you...   \n",
       "\n",
       "                                        short_answer  \n",
       "0      hi there i am rak voice what can i do for you  \n",
       "1  glad that you ask this questions i am an intel...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogues_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation = train_test_split(dialogues_df, \n",
    "                                     test_size=0.1,\n",
    "                                     random_state=42,\n",
    "                                     shuffle=True,\n",
    "                                     stratify=dialogues_df.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 512\n",
    "\n",
    "def tokenize_and_filter(q, a):\n",
    "    tokenized_questions, tokenized_answers = [], []\n",
    "    \n",
    "    for (question, answer) in zip (q, a):\n",
    "        tokenized_question = dialogpt_tokenizer.encode(question)\n",
    "        tokenized_answer = dialogpt_tokenizer.encode(answer)\n",
    "        tokenized_questions.append(tokenized_question)\n",
    "        tokenized_answers.append(tokenized_answer)\n",
    "        \n",
    "    # padding the sequences\n",
    "    tokenized_questions = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_questions, maxlen=MAX_LENGTH, padding=\"post\")\n",
    "    tokenized_answers = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_answers, maxlen=MAX_LENGTH, padding=\"post\")\n",
    "    \n",
    "    return tokenized_questions, tokenized_answers\n",
    "\n",
    "\n",
    "def tokenize_and_filter(q, a):\n",
    "  tokenized_questions, tokenized_answers = [], []\n",
    "  \n",
    "  for (question, answer) in zip(q, a):\n",
    "    # generating sequences\n",
    "    tokenized_question =  biobert_tokenizer.encode(question)\n",
    "    tokenized_answer = biobert_tokenizer.encode(answer)\n",
    "    tokenized_questions.append(tokenized_question)\n",
    "    tokenized_answers.append(tokenized_answer)\n",
    "\n",
    "  # padding the sequences\n",
    "  tokenized_questions = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_questions, maxlen=MAX_LENGTH, padding=\"post\")\n",
    "  tokenized_answers = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_answers, maxlen=MAX_LENGTH, padding=\"post\")\n",
    "  \n",
    "  return tokenized_questions, tokenized_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = train[\"short_question\"]\n",
    "answers = train[\"short_answer\"]\n",
    "labels = train[\"label\"]\n",
    "\n",
    "questions, answers = tokenize_and_filter(questions, answers)\n",
    "\n",
    "train_question_mask = [[1 if token!=0 else 0 for token in question] for question in questions]\n",
    "train_answer_mask = [[1 if token!=0 else 0 for token in answer] for answer in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_questions = validation[\"short_question\"]\n",
    "val_answers = validation[\"short_answer\"]\n",
    "val_labels = validation[\"label\"]\n",
    "\n",
    "val_questions, val_answers = tokenize_and_filter(val_questions, val_answers)\n",
    "\n",
    "val_question_mask = [[1 if token!=0 else 0 for token in question] for question in val_questions]\n",
    "val_answer_mask = [[1 if token!=0 else 0 for token in answer] for answer in val_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        \"question\": questions,\n",
    "        \"answer\": answers,\n",
    "        \"question_mask\": train_question_mask,\n",
    "        \"answer_mask\": train_answer_mask\n",
    "    },\n",
    "    {\n",
    "        \"label\": labels.values\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        \"question\": val_questions,\n",
    "        \"answer\": val_answers,\n",
    "        \"question_mask\": val_question_mask,\n",
    "        \"answer_mask\": val_answer_mask\n",
    "    },\n",
    "    {\n",
    "        \"label\": val_labels.values\n",
    "    },\n",
    "))\n",
    "\n",
    "val_dataset = val_dataset.cache()\n",
    "val_dataset = val_dataset.shuffle(BUFFER_SIZE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "val_dataset = val_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "            self,\n",
    "            name=\"FFN\",\n",
    "            **kwargs):\n",
    "        \"\"\"Simple Dense wrapped with various layers\n",
    "        \"\"\"\n",
    "\n",
    "        super(FFN, self).__init__(name=name, **kwargs)\n",
    "        self.dropout = 0.2\n",
    "        self.ffn_layer = tf.keras.layers.Dense(\n",
    "            units=768,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=tf.keras.initializers.glorot_normal(seed=32),\n",
    "            name=\"FC1\")\n",
    "        \n",
    "\n",
    "    def call(self, inputs):\n",
    "        ffn_embedding = self.ffn_layer(inputs)\n",
    "        ffn_embedding = tf.keras.layers.Dropout(\n",
    "            self.dropout)(ffn_embedding)\n",
    "        ffn_embedding += inputs\n",
    "        \n",
    "        return ffn_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  108310272 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 108,310,272\n",
      "Trainable params: 108,310,272\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "biobert_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogQAModelwithBert(tf.keras.Model):\n",
    "    def __init__(self, trainable=False, name=\"\"):\n",
    "        super(DialogQAModelwithBert, self).__init__(name=name)\n",
    "        self.q_ffn_layer = FFN(name=\"q_ffn\")\n",
    "        self.a_ffn_layer = FFN(name=\"a_ffn\")\n",
    "        self.dialogpt_model = dialogpt_model\n",
    "        self.dialogpt_model.trainable = trainable\n",
    "        self.cos = tf.keras.layers.Dot(axes=1, normalize=True)\n",
    "        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        question_embeddings = self.dialoggpt_model(input_ids=inputs[\"question\"], attention_mask=inputs[\"question_mask\"]).pooler_output\n",
    "        answer_embeddings = self.dialoggpt_model(input_ids=inputs[\"answer\"], attention_mask=inputs[\"answer_mask\"]).pooler_output\n",
    "        q_ffnn = self.q_ffn_layer(question_embeddings)\n",
    "        a_ffnn = self.a_ffn_layer(answer_embeddings)\n",
    "        output = self.cos([q_ffnn,a_ffnn])\n",
    "        \n",
    "        return {\"label\":output}\n",
    "    \n",
    "\n",
    "class MedicalQAModelwithBert(tf.keras.Model):\n",
    "    def __init__(\n",
    "            self,\n",
    "            trainable=False,\n",
    "            name=\"\"):\n",
    "        super(MedicalQAModelwithBert, self).__init__(name=name)\n",
    "\n",
    "        self.q_ffn_layer = FFN(name=\"q_ffn\")\n",
    "        self.a_ffn_layer = FFN(name=\"a_ffn\")\n",
    "        self.biobert_model = biobert_model\n",
    "        self.biobert_model.trainable = trainable\n",
    "        self.cos = tf.keras.layers.Dot(axes=1, normalize=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "      question_embeddings = self.biobert_model(input_ids=inputs[\"question\"], attention_mask=inputs[\"question_mask\"]).pooler_output\n",
    "      answer_embeddings = self.biobert_model(input_ids=inputs[\"answer\"], attention_mask=inputs[\"answer_mask\"]).pooler_output\n",
    "      q_ffnn = self.q_ffn_layer(question_embeddings)\n",
    "      a_ffnn = self.a_ffn_layer(answer_embeddings)\n",
    "      output = self.cos([q_ffnn, a_ffnn])\n",
    "      \n",
    "      return {\"label\":output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Callback(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.history = {\"acc\": []}\n",
    "        \n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.history[\"acc\"].append(logs.get(\"custom_metric_acc\"))\n",
    "        \n",
    "        if (epoch==0) or (logs.get(\"custom_metric_acc\") > self.history([\"acc\"][epoch-1])):\n",
    "            self.model.save_weights(\"dialog_gpt_re\" + str(epoch) + \"/rakchat\" + str(epoch) + \"_\" + str(logs.get(\"custom_metric_acc\")))\n",
    "            \n",
    "        is_nan_values = []\n",
    "        for i in self.model.get_weights():\n",
    "            is_nan_values.append(np.isnan(i).any())\n",
    "            \n",
    "        if (np.array(is_nan_value).any() or (tf.math.is_nan(logs.get(\"loss\"))) or (np.isinf(logs.get(\"loss\")))):\n",
    "            self.model.stop_training = True\n",
    "            \n",
    "\n",
    "class custom_callback(tf.keras.callbacks.Callback):\n",
    "  def on_train_begin(self, logs={}):\n",
    "        ## on begin of training, we are creating a instance varible called history\n",
    "        ## it is a dict with keys [loss, acc, val_loss, val_acc]\n",
    "        self.history={\"acc\": []}\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "        self.history[\"acc\"].append(logs.get(\"custom_metric_acc\"))\n",
    "  \n",
    "        #saving the model if validation accuracy increased from previous epoch\n",
    "        if  (epoch==0) or (logs.get(\"custom_metric_acc\")>self.history[\"acc\"][epoch-1]):\n",
    "          fname = os.path.join(Config.FILES[\"MODEL_DATA_DIR\"], (\"medical_bert_re\" + str(epoch) + \"\\medic\" + str(epoch) + \"_\" + str(logs.get(\"custom_metric_acc\"))))\n",
    "          self.model.save_weights(fname)\n",
    "\n",
    "        is_nan_values=[]\n",
    "        for i in self.model.get_weights():\n",
    "          is_nan_values.append(np.isnan(i).any())\n",
    "\n",
    "        #stopping the training if weights is nan or loss is nan or inf\n",
    "        if (np.array(is_nan_values).any() or (tf.math.is_nan(logs.get(\"loss\"))) or (np.isinf(logs.get(\"loss\")))):\n",
    "          self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "def custom_metric_acc(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, [tf.constant(batch_size)])\n",
    "  y_pred = tf.reshape(y_pred, [tf.constant(batch_size)])\n",
    "  c = tf.constant(0, dtype=\"float32\")\n",
    "  d = tf.cast(tf.math.greater_equal(y_true,c), dtype=\"float32\")\n",
    "  e = tf.cast(tf.math.greater_equal(y_pred,c), dtype=\"float32\")\n",
    "  f = tf.cast(tf.math.equal(d,e), dtype=\"float32\")\n",
    "  g = tf.reduce_sum(f)\n",
    "  h = tf.cast(tf.shape(f), dtype=\"float32\")\n",
    "  i = g/h\n",
    "  return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "55/55 [==============================] - 453s 8s/step - loss: 19.8508 - custom_metric_acc: 1.0000 - val_loss: 20.5369 - val_custom_metric_acc: 1.0000\n",
      "Epoch 2/5\n",
      "55/55 [==============================] - 405s 7s/step - loss: 19.8471 - custom_metric_acc: 1.0000 - val_loss: 16.5184 - val_custom_metric_acc: 1.0000\n",
      "Epoch 3/5\n",
      "55/55 [==============================] - 397s 7s/step - loss: 19.6869 - custom_metric_acc: 1.0000 - val_loss: 19.7678 - val_custom_metric_acc: 1.0000\n",
      "Epoch 4/5\n",
      "55/55 [==============================] - 416s 8s/step - loss: 19.2841 - custom_metric_acc: 1.0000 - val_loss: 21.8491 - val_custom_metric_acc: 1.0000\n",
      "Epoch 5/5\n",
      "55/55 [==============================] - 396s 7s/step - loss: 19.3511 - custom_metric_acc: 1.0000 - val_loss: 20.5121 - val_custom_metric_acc: 1.0000\n",
      "Model: \"\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " q_ffn (FFN)                 multiple                  590592    \n",
      "                                                                 \n",
      " a_ffn (FFN)                 multiple                  590592    \n",
      "                                                                 \n",
      " tf_bert_model (TFBertModel)  multiple                 108310272 \n",
      "                                                                 \n",
      " dot_21 (Dot)                multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109,491,456\n",
      "Trainable params: 109,491,456\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.set_floatx(\"float32\")\n",
    "learning_rate = 5e-6\n",
    "num_epochs = 5\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "medical_qa_model = MedicalQAModelwithBert(trainable=True)\n",
    "medical_qa_model.compile(\n",
    "    optimizer=optimizer, loss=tf.keras.losses.mean_squared_error, metrics=[custom_metric_acc])\n",
    "\n",
    "epochs = num_epochs\n",
    "\n",
    "medical_qa_model.fit(dataset, validation_data=val_dataset, epochs=epochs, callbacks=[custom_callback()])\n",
    "medical_qa_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m medical_qa_model \u001b[39m=\u001b[39m MedicalQAModelwithBert(trainable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m model_fname \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./medical_bert_re0/medic0_1.0.data-00000-of-00001\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m medical_qa_model\u001b[39m.\u001b[39;49mload_weights(model_fname)\n\u001b[0;32m      6\u001b[0m learning_rate \u001b[39m=\u001b[39m \u001b[39m5e-6\u001b[39m\n\u001b[0;32m      7\u001b[0m optimizer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39mlearning_rate)\n",
      "File \u001b[1;32mc:\\Users\\kewjs\\Documents\\02-Self_Learning\\01-Data_Science\\07-Chatbot\\chatbot_venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\kewjs\\Documents\\02-Self_Learning\\01-Data_Science\\07-Chatbot\\chatbot_venv\\lib\\site-packages\\keras\\saving\\legacy\\save.py:476\u001b[0m, in \u001b[0;36mload_weights\u001b[1;34m(model, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[0;32m    471\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[0;32m    472\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`load_weights` requires h5py package when loading weights \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    473\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfrom HDF5. Try installing h5py.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    474\u001b[0m     )\n\u001b[0;32m    475\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model\u001b[39m.\u001b[39m_is_graph_network \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m model\u001b[39m.\u001b[39mbuilt:\n\u001b[1;32m--> 476\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    477\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to load weights saved in HDF5 format into a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    478\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msubclassed Model which has not created its variables yet. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    479\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCall the Model first, then load the weights.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    480\u001b[0m     )\n\u001b[0;32m    481\u001b[0m model\u001b[39m.\u001b[39m_assert_weights_created()\n\u001b[0;32m    482\u001b[0m \u001b[39mwith\u001b[39;00m h5py\u001b[39m.\u001b[39mFile(filepath, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights."
     ]
    }
   ],
   "source": [
    "K.set_floatx(\"float32\")\n",
    "medical_qa_model = MedicalQAModelwithBert(trainable=True)\n",
    "\n",
    "model_fname = \"./medical_bert_re0/medic0_1.0.data-00000-of-00001\"\n",
    "medical_qa_model.load_weights(model_fname)\n",
    "learning_rate = 5e-6\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "medical_qa_model.compile(\n",
    "    optimizer=optimizer, loss=tf.keras.losses.mean_squared_error, metrics=[custom_metric_acc])\n",
    "\n",
    "predicted_labels=[]\n",
    "from tqdm.notebook import tqdm\n",
    "for i in tqdm(range(len(val_questions))):\n",
    "  predicted_labels.append(medical_qa_model.predict({\"question\":np.array([val_questions[i]]),\n",
    "                                                    \"question_mask\":np.array([val_question_mask[i]]),\n",
    "                                                    \"answer\":np.array([val_answers[i]]),\n",
    "                                                    \"answer_mask\":np.array([val_answer_mask[i]])})[\"label\"][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = os.path.join\n",
    "with open('ehealthforumQAs.json') as f1:\n",
    "  ehealth=json.load(f1)[\"data\"]\n",
    "with open('icliniqQAs.json') as f2:\n",
    "  icliniq=json.load(f2)[\"data\"]\n",
    "with open('questionDoctorQAs.json') as f3:\n",
    "  questiondoctor=json.load(f3)[\"data\"]\n",
    "with open('webmdQAs.json') as f4:\n",
    "  webmd=json.load(f4)[\"data\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
